---
phase: 01-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - app/workers/__init__.py
  - app/workers/celery_app.py
  - app/workers/tasks.py
  - app/api/routes/packages.py
  - app/services/post_service.py
  - app/core/models/post_processor.py
  - app/core/constants.py
autonomous: true
requirements: []
must_haves:
  truths:
    - "Engineer uploads a .zip file and receives a 202 response with package_id and job_id"
    - "ZIP validation rejects files with unrecognized extensions and returns friendly error message with expandable detail"
    - "ZIP validation rejects ZIPs with no .SRC file and returns friendly error"
    - "Celery worker picks up the ingestion job and moves package status from pending through validating/storing/parsing to ready"
    - "Engineer can poll GET /api/v1/packages/{id}/status and see the current status of the ingestion job"
    - "Each file from the ZIP is stored in MinIO under packages/{package_id}/{filename}"
    - "Nested directories inside the ZIP are flattened (filename only, directory prefix stripped)"
  artifacts:
    - path: "app/workers/celery_app.py"
      provides: "Celery application configured with Redis broker"
      contains: "Celery"
    - path: "app/workers/tasks.py"
      provides: "Ingestion task skeleton with status tracking"
      contains: "ingest_package"
    - path: "app/api/routes/packages.py"
      provides: "ZIP upload endpoint and status polling endpoint"
      contains: "upload_package"
    - path: "app/core/constants.py"
      provides: "Valid UPG file extensions and error messages"
      contains: "VALID_UPG_EXTENSIONS"
  key_links:
    - from: "app/api/routes/packages.py"
      to: "app/workers/tasks.py"
      via: "Upload endpoint enqueues Celery task"
      pattern: "ingest_package\\.delay"
    - from: "app/workers/tasks.py"
      to: "app/services/storage.py"
      via: "Task reads files from MinIO"
      pattern: "storage"
    - from: "app/workers/tasks.py"
      to: "app/services/post_service.py"
      via: "Task updates package status in PostgreSQL"
      pattern: "update_package_status"
    - from: "app/api/routes/packages.py"
      to: "app/core/constants.py"
      via: "Upload validates against allowed extensions"
      pattern: "VALID_UPG_EXTENSIONS"
---

<objective>
Build the async ingestion pipeline: ZIP file upload, validation, MinIO storage, Celery background task, and job status polling. The parse step inside the Celery task is a stub — it logs "not yet implemented" and marks the package as ready. This skeleton will be filled in by Phase 2 parser work.

Purpose: This is the third success criterion for Phase 1: "An async ingestion job skeleton accepts an uploaded file, enqueues it to Celery/Redis, and returns a job ID." It proves the async infrastructure works end-to-end before any parsing logic is built.

Output: Celery app, ingestion task, ZIP upload endpoint, status polling endpoint, file validation constants.
</objective>

<execution_context>
@C:/Users/CameronCarson/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/CameronCarson/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
@app/config.py
@app/db/models.py
@app/services/storage.py
@app/services/post_service.py
@app/api/routes/packages.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Celery app, constants, and ingestion task skeleton</name>
  <files>
    app/workers/__init__.py
    app/workers/celery_app.py
    app/workers/tasks.py
    app/core/constants.py
  </files>
  <action>
**Create `app/core/constants.py`** — central location for UPG file validation:
```python
VALID_UPG_EXTENSIONS = frozenset({".SRC", ".LIB", ".CTL", ".KIN", ".ATR", ".PINF", ".LNG"})
```
These 7 extensions are confirmed complete from corpus scan (research). No unknown UPG extensions exist.

**Create `app/workers/__init__.py`** — empty module init.

**Create `app/workers/celery_app.py`** — Celery application factory:
- Import Celery
- Create app: `celery_app = Celery("veripost", broker=settings.celery_broker_url, backend=settings.celery_result_backend)`
- Configure: `task_serializer="json"`, `result_serializer="json"`, `accept_content=["json"]`, `timezone="UTC"`
- Auto-discover tasks from `app.workers.tasks`
- IMPORTANT: The `worker` service in docker-compose.yml references `app.workers.celery_app:celery_app` — the module path must match

**Create `app/workers/tasks.py`** — ingestion task skeleton per research Pattern 2:

```python
@celery_app.task(bind=True, name="ingest_package")
def ingest_package(self, package_id: str) -> dict:
    """
    Skeleton ingestion task. Processes a ZIP package that was already
    uploaded to MinIO by the API route. Status flow:
    pending -> validating -> storing -> parsing (stub) -> ready | error
    """
```

The task must:
1. Update status to "validating" — verify all files exist in MinIO under `packages/{package_id}/`
2. Update status to "storing" — confirm file integrity (files already in MinIO from upload handler, this step verifies they're accessible)
3. Update status to "parsing" — **STUB**: log "Parse step not yet implemented (Phase 2)", set section_count=0
4. Update status to "ready" — mark package as successfully ingested
5. On any exception: update status to "error" with friendly `error_message` and technical `error_detail`

IMPORTANT: The Celery worker runs in a separate container from the API. It cannot use async/await (Celery tasks are synchronous). Use `boto3` (not aiobotocore) for synchronous MinIO access inside the worker, OR use synchronous SQLAlchemy session. The simplest approach: use `sqlalchemy.create_engine` (sync) with `psycopg2` driver for the worker's database access, and `boto3` for MinIO. Add `psycopg2-binary` and `boto3` to pyproject.toml dependencies.

Alternative approach (if cleaner): use `celery_app.conf.update()` and have the task call synchronous helper functions. The key constraint is that Celery tasks MUST be synchronous — do not try to run an async event loop inside a Celery task.

**Update `pyproject.toml`** — add `psycopg2-binary>=2.9` and `boto3>=1.34` to dependencies (for synchronous Celery worker access).
  </action>
  <verify>
With Docker services running:
- `docker compose up -d` — worker container starts without import errors
- `docker compose logs worker` — shows Celery startup banner with `ingest_package` task registered
- No async/await in tasks.py (Celery compatibility)
  </verify>
  <done>
Celery worker starts inside Docker, connects to Redis broker, and registers the `ingest_package` task. Task skeleton handles the full status flow (pending -> validating -> storing -> parsing -> ready) with the parse step stubbed.
  </done>
</task>

<task type="auto">
  <name>Task 2: ZIP upload endpoint and job status polling</name>
  <files>
    app/api/routes/packages.py
    app/services/post_service.py
    app/core/models/post_processor.py
  </files>
  <action>
**Expand `app/api/routes/packages.py`** — add ZIP upload and status endpoints:

**`POST /api/v1/packages/upload`** (202 Accepted):
1. Validate the uploaded file is a .zip: if not, return 400 with ErrorResponse ("Please upload a .zip file containing your post processor package.")
2. Read ZIP bytes into memory
3. Call `validate_zip_contents(content)` — validates:
   - ZIP is not empty
   - All files have extensions in `VALID_UPG_EXTENSIONS` (per constants.py). Extension check is case-insensitive.
   - At least one .SRC file exists
   - Handle nested directories per research Pitfall 5: strip directory prefixes, use filename only
4. If validation fails, return 422 with ErrorResponse:
   - `message`: friendly English summary (see research Pattern 4 error table)
   - `detail`: technical details listing each rejected file
5. Generate package_id (UUID)
6. Extract ZIP and upload each file to MinIO under `packages/{package_id}/{filename}` (flattened — no directory structure)
7. Create PostPackage record in PostgreSQL with status="pending", file_count, minio_prefix
8. Create PostFile records for each file (filename, extension, minio_key, size_bytes, content_hash via SHA-256)
9. Enqueue `ingest_package.delay(str(package_id))`
10. Return 202 with `{"package_id": str, "job_id": str, "status": "pending"}`

**`GET /api/v1/packages/{package_id}/status`** (200):
- Query PostPackage from PostgreSQL
- Return: `{"package_id": str, "status": str, "error_message": str|null, "error_detail": str|null, "file_count": int, "section_count": int|null}`
- If package not found: 404

**Error response model** (add to `app/core/models/post_processor.py`):
```python
class ErrorResponse(BaseModel):
    message: str       # Friendly plain-English message
    detail: str | None = None  # Technical detail for expandable section
    code: str | None = None    # Machine-readable error code
```
This is the platform-wide error pattern per user decision. Use it for ALL error responses going forward.

**`validate_zip_contents()`** function — place in `app/services/post_service.py` or a new `app/services/validation.py`:
- Uses `zipfile.ZipFile(io.BytesIO(content))` to read ZIP
- Flattens nested directories (strips path prefixes, keeps filename only) per research Pitfall 5
- Skips `__MACOSX/` entries and directory-only entries
- Checks each file extension against `VALID_UPG_EXTENSIONS`
- Checks for at least one .SRC file
- Returns list of error strings (empty = valid)

**Update `app/services/post_service.py`** — add functions needed by the upload route:
- `create_package_with_files(db, package_id, name, files_metadata, minio_prefix)` — creates both PostPackage and PostFile records in a single transaction
  </action>
  <verify>
With all Docker services running:

1. Upload a test ZIP containing valid UPG files:
   - Create a test ZIP with a .SRC and .LIB file
   - `curl -X POST -F "file=@test_package.zip" http://localhost:8000/api/v1/packages/upload`
   - Returns 202 with package_id and job_id

2. Poll status:
   - `curl http://localhost:8000/api/v1/packages/{package_id}/status`
   - Status progresses from "pending" to "ready" within a few seconds

3. Verify files in MinIO:
   - Check MinIO console at http://localhost:9001 — see files under packages/{package_id}/

4. Test rejection cases:
   - Upload a ZIP with a .NC file → 422 with friendly error about unrecognized file type
   - Upload a ZIP with no .SRC file → 422 with friendly error about missing SRC
   - Upload a non-ZIP file → 400 with friendly error asking for .zip

5. Test nested directory handling:
   - Create a ZIP where files are inside a subdirectory (e.g., `HAAS_VF-4/HAAS_VF-4.SRC`)
   - Upload it → files stored flattened as `HAAS_VF-4.SRC` (no directory prefix)

6. Download a stored file:
   - `curl http://localhost:8000/api/v1/packages/{package_id}/files/{file_id}/download`
   - Returns original bytes
  </verify>
  <done>
ZIP upload endpoint accepts .zip files, validates contents against UPG extension whitelist, stores files in MinIO with package-level organization, creates database records, and enqueues Celery ingestion task. Status polling endpoint returns current job status. Error responses follow the "friendly message + expandable detail" pattern. Nested ZIPs are flattened correctly.
  </done>
</task>

</tasks>

<verification>
1. End-to-end: upload ZIP → 202 response → poll status → reaches "ready" → download file → original bytes match
2. Celery worker logs show task execution: validating → storing → parsing (stub) → ready
3. Invalid ZIP rejected with friendly error messages
4. MinIO shows files organized by package: packages/{id}/{filename}
5. PostgreSQL has matching records in post_packages and post_files tables
</verification>

<success_criteria>
- ZIP upload returns 202 with job_id
- Celery task processes the package asynchronously (visible in worker logs)
- Status endpoint tracks progress from pending through ready
- Parse step is a documented stub (logs "Phase 2 will implement")
- File validation enforces the 7 known UPG extensions
- Error messages are friendly with expandable technical details
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
